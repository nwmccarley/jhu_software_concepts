1. Nickolas Mccarley nmccarl1@jh.edu
2. Module 2 we are tasked with scraping and cleaning data from the Grad Cafe Website and returning at least 10000 entries to a JSON File. Due 6/1/2025
3. Approach for my approach I used regex, Beautifal Soup, and URLLib to handle the program. The package contains to programs clean.py and scrape.py(the package to be ran) it also contains applicant_data.json, requirements.txt, and screenshot.jpg(a screenshot of the robots.txt of the Grad Cafe found at https://www.thegradcafe.com/robots.txt )
For compliance with the robots.txt page it states that for User-agent /cgi-bin/ and/index-ad-test.php are dissalowed. My program does not violate it because it only deals with the main gradcafe.com link and the survey page. No use is made of the forbidden sections. 
This program works by first using urllib to import the html into Beatiful Soup. Soup then finds all tr and td tags in the html to identify where the wanted information is stored in rows. The data is cleaned using the clean data function.
Clean data uses regex to to find applicable tags and trends in the html to print it in a readable format. First there is a basic format based on institution,degree,date added, and decision. Then there are sections to parse the other optional fields based on what can possibly be contained within them. Most are based on what could be expected for the content, except for comments which is based on the paragraph object they always belong to, and the link to mor info which is based on the actual more info link. All the field are then returned if they exist or "None" is displayed if they do not.
Finally the files are then saved and loaded into a json file back in scrape.py into applicant.json. At the bottom of scrape.py the number of pages to be scrapped can be modified.
4. No bugs are known but since this is a large dataset the program does take a few minutes to run
